{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential,model_from_json\n",
    "from keras.layers.core import Dense,Flatten\n",
    "from keras.optimizers import sgd\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniProject #3: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: $E_p$ is the expectation under probability $p$. Please justify each of your answer and widely comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a reinforcement learning algorithm, we modelize each step $t$ as an action $a_t$ obtained from a state $s_t$, i.e. $\\{(a_{t},s_{t})_{t\\leq T}\\}$ having the Markov property. We consider a discount factor $\\gamma \\in [0,1]$ that ensures convergence. The goal is to find among all the policies $\\pi$, one that maximizes the expected reward:\n",
    "\n",
    "\\begin{equation*}\n",
    "R(\\pi)=\\sum_{t\\leq T}E_{p^{\\pi}}[\\gamma^t r(s_{t},a_{t})] \\> ,\n",
    "\\end{equation*}\n",
    "\n",
    "where: \n",
    "\\begin{equation*}p^{\\pi}(a_{0},a_{1},s_{1},...,a_{T},s_{T})=p(a_{0})\\prod_{t=1}^{T}\\pi(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t}) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "We note the $Q$-function:\n",
    "\n",
    "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, the optimal Q function is:\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "In this project, we will apply the deep reinforcement learning techniques to a simple game: an agent will have to learn from scratch a policy that will permit it maximizing a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment, the agent and the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, act):\n",
    "        \"\"\"\n",
    "        One can act on the environment and obtain its reaction:\n",
    "        - the new state\n",
    "        - the reward of the new state\n",
    "        - should we continue the game?\n",
    "\n",
    "        :return: state, reward, game_over\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinitialize the environment to a random state and returns\n",
    "        the original state\n",
    "\n",
    "        :return: state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Visualize in the console or graphically the current state\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
    "\n",
    "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
    "\n",
    "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
    "\n",
    "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, epsilon=0.1, n_action=4):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_action = n_action\n",
    "    \n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "\n",
    "    def act(self,s,train=True):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if train:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "            else:\n",
    "                a = self.learned_act(s)\n",
    "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learned_act(self,s):\n",
    "        \"\"\" Act via the policy of the agent, from a given state s\n",
    "        it proposes an action a\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reinforce(self, s, n_s, a, r, game_over_):\n",
    "        \"\"\" This function is the core of the learning algorithm. \n",
    "        It takes as an input the current state s_, the next state n_s_\n",
    "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
    "        \n",
    "        Its goal is to learn a policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" This function returns basic stats if applicable: the\n",
    "        loss and/or the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" This function allows to restore a model\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 1__:\n",
    "Explain the function act. Why is ```epsilon``` essential?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning problems raise a common issue know as Exploration/Exploitation dilemma. This is widely discussed in multi armed bandit problems. On the one hand in $\\texttt{act}$ we want to achieve the most rewarding action from what we know at the time, this is exploitation. On the other hand we also want to learn about more rewarding actions, this is exploration. Thus at any time we take the most rewarding action is the space of explored actions. If we do not explore i.e do not use $\\epsilon$ to take random actions, we take make our decision according to very small subset of all possible actions. The odds that this small subset contains the optimal action are thus minimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Agent``` and the ```Environment``` work in an interlaced way as in the following (take some time to understand this code as it is the core of the project)\n",
    "\n",
    "```python\n",
    "\n",
    "epoch = 300\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "# Number of won games\n",
    "score = 0\n",
    "loss = 0\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    # At each epoch, we restart to a fresh game and get the initial state\n",
    "    state = env.reset()\n",
    "    # This assumes that the games will end\n",
    "    game_over = False\n",
    "\n",
    "    win = 0\n",
    "    lose = 0\n",
    "    \n",
    "    while not game_over:\n",
    "        # The agent performs an action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Apply an action to the environment, get the next state, the reward\n",
    "        # and if the games end\n",
    "        prev_state = state\n",
    "        state, reward, game_over = env.act(action)\n",
    "\n",
    "        # Update the counters\n",
    "        if reward > 0:\n",
    "            win = win + reward\n",
    "        if reward < 0:\n",
    "            lose = lose -reward\n",
    "\n",
    "        # Apply the reinforcement strategy\n",
    "        loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "    # Save as a mp4\n",
    "    if e % 10 == 0:\n",
    "        env.draw(e)\n",
    "\n",
    "    # Update stats\n",
    "    score += win-lose\n",
    "\n",
    "    print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "          .format(e, epoch, loss, win, lose, win-lose))\n",
    "    agent.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The game, *eat cheese*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
    "\n",
    "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        reward = self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((\n",
    "                               self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following elements are important because they correspond to the hyper parameters for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "size = 13\n",
    "T=200\n",
    "temperature=0.3\n",
    "epochs_train=30 # set small when debugging\n",
    "epochs_test=30 # set small when debugging\n",
    "\n",
    "# display videos\n",
    "def display_videos(name):\n",
    "    video = io.open(name, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return '''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__ Explain the use of the arrays ```position``` and ```board```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\texttt{position}$ is a an array containing the rat's position on the board whereas $\\texttt{board}$ is a array containing the cheese's and poison's position. They are updated at each iterations. Moreover they are of length grid_size + 4 to compensate the rat's vision of 2. This two arrays at time $t$ define the state $s_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 3__ Implement a random Agent (only ```learned_act``` needs to be implemented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(RandomAgent, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        \n",
    "        return np.random.randint(0, self.n_action, size=1)[0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 4__ Visualize the game moves. You need to fill in the following function for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,env,epochs,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "        \n",
    "    for e in range(epochs):\n",
    "        state = env.reset()\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        #Save as a mp4\n",
    "        env.draw(prefix+str(e))\n",
    "\n",
    "            # Update stats\n",
    "        score = score + win-lose\n",
    "\n",
    "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
    "              .format(win, lose, score/(1+e)))\n",
    "        \n",
    "    print('Final score: '+str(score/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 10.0/11.0. Average score (-1.0)\n",
      "Win/lose count 10.0/13.0. Average score (-2.0)\n",
      "Win/lose count 7.0/18.0. Average score (-5.0)\n",
      "Win/lose count 5.5/14.0. Average score (-5.875)\n",
      "Win/lose count 9.0/19.0. Average score (-6.7)\n",
      "Win/lose count 12.0/13.0. Average score (-5.75)\n",
      "Win/lose count 5.5/10.0. Average score (-5.571428571428571)\n",
      "Win/lose count 11.0/11.0. Average score (-4.875)\n",
      "Win/lose count 11.5/18.0. Average score (-5.055555555555555)\n",
      "Win/lose count 10.0/14.0. Average score (-4.95)\n",
      "Win/lose count 8.5/20.0. Average score (-5.545454545454546)\n",
      "Win/lose count 11.0/14.0. Average score (-5.333333333333333)\n",
      "Win/lose count 9.0/11.0. Average score (-5.076923076923077)\n",
      "Win/lose count 5.5/5.0. Average score (-4.678571428571429)\n",
      "Win/lose count 9.0/9.0. Average score (-4.366666666666666)\n",
      "Win/lose count 13.5/22.0. Average score (-4.625)\n",
      "Win/lose count 10.5/10.0. Average score (-4.323529411764706)\n",
      "Win/lose count 10.0/18.0. Average score (-4.527777777777778)\n",
      "Win/lose count 12.0/11.0. Average score (-4.2368421052631575)\n",
      "Win/lose count 8.0/20.0. Average score (-4.625)\n",
      "Win/lose count 9.5/7.0. Average score (-4.285714285714286)\n",
      "Win/lose count 8.0/15.0. Average score (-4.409090909090909)\n",
      "Win/lose count 7.5/9.0. Average score (-4.282608695652174)\n",
      "Win/lose count 8.0/9.0. Average score (-4.145833333333333)\n",
      "Win/lose count 10.0/13.0. Average score (-4.1)\n",
      "Win/lose count 7.0/18.0. Average score (-4.365384615384615)\n",
      "Win/lose count 8.0/4.0. Average score (-4.055555555555555)\n",
      "Win/lose count 9.0/11.0. Average score (-3.982142857142857)\n",
      "Win/lose count 12.0/15.0. Average score (-3.9482758620689653)\n",
      "Win/lose count 12.5/12.0. Average score (-3.8)\n",
      "Final score: -3.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the game\n",
    "env = Environment(grid_size=size, max_time=T,temperature=temperature)\n",
    "\n",
    "# Initialize the agent!\n",
    "agent = RandomAgent()\n",
    "\n",
    "test(agent,env,epochs_test,prefix='random')\n",
    "HTML(display_videos('random0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume here that $T=\\infty$.\n",
    "\n",
    "***\n",
    "__Question 5__ Let $\\pi$ be a policy, show that:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{equation*}\n",
    "\n",
    "Then, show that for the optimal policy $\\pi^*$ (we assume its existence), the following holds: \n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{equation*}\n",
    "Finally, deduce that a plausible objective is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideas here is to take out of the sum the first reward that is certain as we know starting state and action. Then rewrite the last term as $Q^\\pi(s_1,a_1)$: \n",
    "\\begin{align}\n",
    "Q^\\pi(s,a)&=E_{p^{\\pi}}[\\sum_{0 \\leq t}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \n",
    "\\\\\n",
    "&= r(s,a) + E_{p^{\\pi}}[\\sum_{1\\leq t}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a]\n",
    "\\\\\n",
    "&= r(s,a) + \\gamma\\sum_{s',a'}p(s_1=s'| s,a)\\pi(a'|s')E_{p^{\\pi}}[\\sum_{1\\leq t}\\gamma^{t-1}r(s_{t},a_{t})|s_{1}=s',a_{1}=a']\n",
    "\\end{align}\n",
    "\n",
    "by reindexing the last sum we recognize $Q^\\pi(s',a')$. Moreover the sum is simply the expectation for $(s',a') \\sim p(.|s,a)$. Thus we obtain :\n",
    "$$Q^\\pi(s,a) = r(s,a)+ E_{(s',a')\\sim p(.|s,a)}[\\gamma Q^{\\pi}(s',a')]$$\n",
    "\n",
    "With the same idea for Q^* we go from the initial formula and take out first term. Then we separate policy $\\pi$ into a choice for on $a'|s,a$ and another policy $\\pi'$ from  intial state actions $a',s'$:\n",
    "\\begin{align}\n",
    "Q^*(s,a)&=\\max_{\\pi}E_{p^{\\pi}}[\\sum_{0 \\leq t}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \n",
    "\\\\\n",
    "&= r(s,a) + \\max_{\\pi}E_{p^{\\pi}}[\\sum_{1\\leq t}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a]\n",
    "\\\\\n",
    "&= r(s,a) + \\max_{\\pi',a'}\\gamma\\sum_{s'}p(s_1=s'| s,a)\\pi(a'|s')E_{p^{\\pi'}}[\\sum_{1\\leq t}\\gamma^{t-1}r(s_{t},a_{t})|s_{1}=s',a_{1}=a']\n",
    "\\\\\n",
    "&= r(s,a) + \\max_{a'}\\gamma\\sum_{s'}p(s_1=s'| s,a)\\pi ^* (a'|s')\\max_{\\pi'} \\gamma Q^{\\pi'}(s',a')\n",
    "\\\\\n",
    "&=r(s,a) + E_{s'\\sim \\pi^*(.|s,a)}[\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{align}\n",
    "\n",
    "Finally once we have these two expression. A plausible loss function would be the MSE of the current policy with the optimal policy thus:\n",
    "\\begin{align}\n",
    "L(\\theta) &= ||Q^*(s,a,\\theta)-Q(s,a,\\theta)||^2\n",
    "\\\\\n",
    "&= || r(s,a) + E_{s'\\sim \\pi^*(.|s,a)}[\\gamma\\max_{a'}Q^{*}(s',a')] -Q(s,a,\\theta)||^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
    "\n",
    "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
    "\n",
    "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
    "\n",
    "3. Store $(s_t,a_t,s_{t+1})$;\n",
    "\n",
    "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
    "\n",
    "***\n",
    "__Question 6__ Implement the class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. Specify a maximum memory size to avoid side effects. You can for example use a ```list()``` and set by default ```max_memory=100```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "\n",
    "    def remember(self, m):\n",
    "        self.memory.append(m)\n",
    "\n",
    "    def random_access(self):\n",
    "        index =  np.random.randint(0,len(self.memory))\n",
    "        return self.memory[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The pipeline we will use for training is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "            #agent.epsilon*=0.95\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 7__ Implement the DQN training algorithm using a cascade of fully connected layers. You can use different learning rate, batch size or memory size parameters. In particular, the loss might oscillate while the player will start to win the games. You have to find a good criterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Agent):\n",
    "    def __init__(self, grid_size,  epsilon = 0.1, memory_size=100, batch_size = 16,n_state=2):\n",
    "        super(DQN, self).__init__(epsilon = epsilon)\n",
    "\n",
    "        # Discount for Q learning\n",
    "        self.discount = 0.99\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # number of state\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Memory\n",
    "        self.memory = Memory(memory_size)\n",
    "        \n",
    "        # Batch size when learning\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        return np.argmax(self.model.predict(np.array([s])))\n",
    "\n",
    "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
    "        # Two steps: first memorize the states, second learn from the pool\n",
    "\n",
    "        self.memory.remember([s_, n_s_, a_, r_, game_over_])\n",
    "        \n",
    "        input_states = np.zeros((self.batch_size, 5,5,self.n_state))\n",
    "        target_q = np.zeros((self.batch_size, 4))\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            s_, n_s_, a_, r_, game_over_ = self.memory.random_access()\n",
    "            if game_over_:\n",
    "                ######## FILL IN\n",
    "                input_states[i] = s_\n",
    "                target_q[i] = self.model.predict(np.array([s_]))\n",
    "                target_q[i][a_] = r_\n",
    "            else:\n",
    "\n",
    "                input_states[i] = s_\n",
    "                target_q[i] = self.model.predict(np.array([s_]))\n",
    "                target_q[i][a_] = r_ + self.discount*np.max(self.model.predict(np.array([n_s_])))\n",
    "                ######## FILL IN\n",
    "        ######## FILL IN\n",
    "        # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
    "        \n",
    "        #target_q = np.clip(target_q, -3, 3)\n",
    "        l = self.model.train_on_batch(input_states, target_q)\n",
    "\n",
    "\n",
    "        return l\n",
    "\n",
    "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
    "        self.model.save_weights(name_weights, overwrite=True)\n",
    "        with open(name_model, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "            \n",
    "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
    "        with open(name_model, \"r\") as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "        model.load_weights(name_weights)\n",
    "        model.compile(\"sgd\", \"mse\")\n",
    "        self.model = model\n",
    "\n",
    "            \n",
    "class DQN_FC(DQN):\n",
    "    def __init__(self, *args, lr=0.1,**kwargs):\n",
    "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
    "        \n",
    "        # NN Model\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(5,5,self.n_state)))\n",
    "        model.add(Dense(100,activation ='relu'))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(4, activation='relu'))\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        \n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_46 (Flatten)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 15,604\n",
      "Trainable params: 15,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 000/010 | Loss 0.0035 | Win/lose count 2.0/6.0 (-4.0)\n",
      "Epoch 001/010 | Loss 0.0061 | Win/lose count 3.5/2.0 (1.5)\n",
      "Epoch 002/010 | Loss 0.0195 | Win/lose count 3.5/4.0 (-0.5)\n",
      "Epoch 003/010 | Loss 0.3828 | Win/lose count 3.0/6.0 (-3.0)\n",
      "Epoch 004/010 | Loss 0.0098 | Win/lose count 2.5/6.0 (-3.5)\n",
      "Epoch 005/010 | Loss 0.0078 | Win/lose count 2.5/1.0 (1.5)\n",
      "Epoch 006/010 | Loss 0.0000 | Win/lose count 0.5/4.0 (-3.5)\n",
      "Epoch 007/010 | Loss 0.0039 | Win/lose count 1.5/2.0 (-0.5)\n",
      "Epoch 008/010 | Loss 0.0078 | Win/lose count 1.0/4.0 (-3.0)\n",
      "Epoch 009/010 | Loss 0.0117 | Win/lose count 1.0/2.0 (-1.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent, env, epochs_train, prefix='fc_train')\n",
    "HTML(display_videos('fc_train10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 8__ Implement the DQN training algorithm using a CNN (for example, 2 convolutional layers and one final fully connected layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32,2,padding='same',activation='relu',input_shape=(5,5,self.n_state)))\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Conv2D(64,2,activation='relu',padding='same'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64,activation ='relu'))\n",
    "        model.add(Dense(4, activation='relu'))\n",
    "        \n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        print(model.summary())\n",
    "\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_73 (Conv2D)           (None, 5, 5, 32)          288       \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 5, 5, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 25,252\n",
      "Trainable params: 25,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 000/010 | Loss 0.0032 | Win/lose count 2.5/4.0 (-1.5)\n",
      "Epoch 001/010 | Loss 0.0030 | Win/lose count 3.0/1.0 (2.0)\n",
      "Epoch 002/010 | Loss 0.0094 | Win/lose count 3.0/6.0 (-3.0)\n",
      "Epoch 003/010 | Loss 0.0118 | Win/lose count 3.5/3.0 (0.5)\n",
      "Epoch 004/010 | Loss 0.0007 | Win/lose count 2.5/2.0 (0.5)\n",
      "Epoch 005/010 | Loss 0.0167 | Win/lose count 5.5/5.0 (0.5)\n",
      "Epoch 006/010 | Loss 0.0044 | Win/lose count 2.0/4.0 (-2.0)\n",
      "Epoch 007/010 | Loss 0.0104 | Win/lose count 1.5/2.0 (-0.5)\n",
      "Epoch 008/010 | Loss 0.0190 | Win/lose count 3.0/2.0 (1.0)\n",
      "Epoch 009/010 | Loss 0.0023 | Win/lose count 2.5/5.0 (-2.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent,env,epochs_train,prefix='cnn_train')\n",
    "HTML(display_videos('cnn_train10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 9__ Test both algorithms and compare their performances. Which issue(s) do you observe? Observe also different behaviors by changing the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of the CNN\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_77 (Conv2D)           (None, 5, 5, 32)          288       \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 5, 5, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 25,252\n",
      "Trainable params: 25,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Win/lose count 1.5/4.0. Average score (-2.5)\n",
      "Win/lose count 2.5/3.0. Average score (-1.5)\n",
      "Win/lose count 1.5/1.0. Average score (-0.8333333333333334)\n",
      "Win/lose count 2.5/2.0. Average score (-0.5)\n",
      "Win/lose count 2.0/0. Average score (0.0)\n",
      "Win/lose count 4.5/1.0. Average score (0.5833333333333334)\n",
      "Win/lose count 1.0/0. Average score (0.6428571428571429)\n",
      "Win/lose count 1.0/3.0. Average score (0.3125)\n",
      "Win/lose count 1.5/4.0. Average score (0.0)\n",
      "Win/lose count 1.5/2.0. Average score (-0.05)\n",
      "Win/lose count 3.0/1.0. Average score (0.13636363636363635)\n",
      "Win/lose count 4.0/7.0. Average score (-0.125)\n",
      "Win/lose count 1.5/2.0. Average score (-0.15384615384615385)\n",
      "Win/lose count 4.5/1.0. Average score (0.10714285714285714)\n",
      "Win/lose count 3.0/2.0. Average score (0.16666666666666666)\n",
      "Win/lose count 4.5/4.0. Average score (0.1875)\n",
      "Win/lose count 2.0/4.0. Average score (0.058823529411764705)\n",
      "Win/lose count 2.5/5.0. Average score (-0.08333333333333333)\n",
      "Win/lose count 2.5/4.0. Average score (-0.15789473684210525)\n",
      "Win/lose count 3.5/3.0. Average score (-0.125)\n",
      "Win/lose count 4.5/0. Average score (0.09523809523809523)\n",
      "Win/lose count 3.0/3.0. Average score (0.09090909090909091)\n",
      "Win/lose count 2.0/2.0. Average score (0.08695652173913043)\n",
      "Win/lose count 2.5/3.0. Average score (0.0625)\n",
      "Win/lose count 3.0/2.0. Average score (0.1)\n",
      "Win/lose count 1.5/2.0. Average score (0.07692307692307693)\n",
      "Win/lose count 3.0/1.0. Average score (0.14814814814814814)\n",
      "Win/lose count 0.5/2.0. Average score (0.08928571428571429)\n",
      "Win/lose count 1.5/2.0. Average score (0.06896551724137931)\n",
      "Win/lose count 4.0/5.0. Average score (0.03333333333333333)\n",
      "Final score: 0.03333333333333333\n",
      "Test of the FC\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_51 (Flatten)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 15,604\n",
      "Trainable params: 15,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Win/lose count 2.0/9.0. Average score (-7.0)\n",
      "Win/lose count 1.0/2.0. Average score (-4.0)\n",
      "Win/lose count 3.5/6.0. Average score (-3.5)\n",
      "Win/lose count 2.5/0. Average score (-2.0)\n",
      "Win/lose count 3.5/2.0. Average score (-1.3)\n",
      "Win/lose count 4.5/5.0. Average score (-1.1666666666666667)\n",
      "Win/lose count 2.0/0. Average score (-0.7142857142857143)\n",
      "Win/lose count 1.0/2.0. Average score (-0.75)\n",
      "Win/lose count 2.0/1.0. Average score (-0.5555555555555556)\n",
      "Win/lose count 0.5/3.0. Average score (-0.75)\n",
      "Win/lose count 5.0/2.0. Average score (-0.4090909090909091)\n",
      "Win/lose count 3.0/2.0. Average score (-0.2916666666666667)\n",
      "Win/lose count 3.5/5.0. Average score (-0.38461538461538464)\n",
      "Win/lose count 3.5/6.0. Average score (-0.5357142857142857)\n",
      "Win/lose count 1.5/2.0. Average score (-0.5333333333333333)\n",
      "Win/lose count 2.5/3.0. Average score (-0.53125)\n",
      "Win/lose count 4.5/6.0. Average score (-0.5882352941176471)\n",
      "Win/lose count 2.5/5.0. Average score (-0.6944444444444444)\n",
      "Win/lose count 1.5/4.0. Average score (-0.7894736842105263)\n",
      "Win/lose count 1.5/4.0. Average score (-0.875)\n",
      "Win/lose count 2.0/3.0. Average score (-0.8809523809523809)\n",
      "Win/lose count 2.0/4.0. Average score (-0.9318181818181818)\n",
      "Win/lose count 3.5/2.0. Average score (-0.8260869565217391)\n",
      "Win/lose count 3.0/4.0. Average score (-0.8333333333333334)\n",
      "Win/lose count 3.5/7.0. Average score (-0.94)\n",
      "Win/lose count 2.0/2.0. Average score (-0.9038461538461539)\n",
      "Win/lose count 2.5/0. Average score (-0.7777777777777778)\n",
      "Win/lose count 6.0/7.0. Average score (-0.7857142857142857)\n",
      "Win/lose count 2.0/3.0. Average score (-0.7931034482758621)\n",
      "Win/lose count 4.0/5.0. Average score (-0.8)\n",
      "Final score: -0.8\n"
     ]
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
    "\n",
    "print('Test of the CNN')\n",
    "agent_cnn = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='cnn_trainmodel.h5',name_model='cnn_trainmodel.json')\n",
    "test(agent_cnn,env,epochs_test,prefix='cnn_test')\n",
    "\n",
    "print('Test of the FC')\n",
    "agent_fc = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='fc_trainmodel.h5',name_model='fc_trainmodel.json')\n",
    "test(agent_fc,env,epochs_test,prefix='fc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('cnn_test10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('fc_test10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models perform poorly. Both doesn't explore enough and go back to position they have already been to. With low temperature Convolutionnal model performs better where at high temperature it's the other way around. Globally we need to improve our model by increasing exploration at train time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
    "1. Incorporating a decreasing $\\epsilon$-greedy exploration. You can use the method ```set_epsilon```\n",
    "2. Append via the environment a new state that describes if a cell has been visited or not\n",
    "\n",
    "***\n",
    "__Question 10__ Design a new ```train_explore``` function and environment class ```EnvironmentExploring``` to tackle the issue of exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explore(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "    eps = agent.epsilon\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "        win = 0\n",
    "        lose = 0\n",
    "        malus = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action,train=True)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward >= 0:\n",
    "                win = win + reward\n",
    "            elif reward <= -0.5:\n",
    "                lose = lose - reward\n",
    "            else :\n",
    "                malus -= reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "        \n",
    "        # We decrease geometrically espilon which allows a higher value at start\n",
    "        agent.set_epsilon(eps**e)\n",
    "        \n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose/malus count {}/{}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, malus, win-lose-malus))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
    "        \n",
    "class EnvironmentExploring(object):\n",
    "    \n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "        self.malus_position = np.ones((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action, train=False):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "            \n",
    "        reward = 0\n",
    "        if train:\n",
    "            reward = -self.malus_position[self.x, self.y]\n",
    "            \n",
    "        # We add a malus for passing by an already visited spot.\n",
    "        # This penalty increase linearly up to a threshold\n",
    "        if self.malus_position[self.x, self.y] < 0.4:\n",
    "            self.malus_position[self.x, self.y] += 0.05\n",
    "            \n",
    "        reward = reward + self.board[self.x, self.y]\n",
    "        self.t = self.t + 1\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "        self.malus_position = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        \n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_91 (Conv2D)           (None, 5, 5, 32)          416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 2, 2, 64)          8256      \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 25,380\n",
      "Trainable params: 25,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 000/030 | Loss 0.0461 | Win/lose count 8.5/48.60000000000004 (-40.10000000000004)\n",
      "Epoch 001/030 | Loss 0.0315 | Win/lose count 11.5/47.30000000000003 (-35.80000000000003)\n",
      "Epoch 002/030 | Loss 0.0222 | Win/lose count 9.0/46.60000000000005 (-37.60000000000005)\n",
      "Epoch 003/030 | Loss 0.0363 | Win/lose count 6.0/63.4 (-57.4)\n",
      "Epoch 004/030 | Loss 0.0631 | Win/lose count 9.0/55.60000000000001 (-46.60000000000001)\n",
      "Epoch 005/030 | Loss 0.0410 | Win/lose count 6.5/59.50000000000001 (-53.00000000000001)\n",
      "Epoch 006/030 | Loss 0.0289 | Win/lose count 3.0/76.89999999999999 (-73.89999999999999)\n",
      "Epoch 007/030 | Loss 0.0350 | Win/lose count 5.5/71.39999999999999 (-65.89999999999999)\n",
      "Epoch 008/030 | Loss 0.0428 | Win/lose count 4.5/72.7 (-68.2)\n",
      "Epoch 009/030 | Loss 0.0598 | Win/lose count 6.5/70.2 (-63.7)\n",
      "Epoch 010/030 | Loss 0.0383 | Win/lose count 3.5/70.2 (-66.7)\n",
      "Epoch 011/030 | Loss 0.0247 | Win/lose count 5.5/76.0 (-70.5)\n",
      "Epoch 012/030 | Loss 0.0359 | Win/lose count 4.5/73.80000000000001 (-69.30000000000001)\n",
      "Epoch 013/030 | Loss 0.0383 | Win/lose count 8.5/75.4 (-66.9)\n",
      "Epoch 014/030 | Loss 0.0383 | Win/lose count 3.0/73.2 (-70.2)\n",
      "Epoch 015/030 | Loss 0.0459 | Win/lose count 3.0/78.4 (-75.4)\n",
      "Epoch 016/030 | Loss 0.0333 | Win/lose count 5.0/75.60000000000002 (-70.60000000000002)\n",
      "Epoch 017/030 | Loss 0.0370 | Win/lose count 2.5/86.8 (-84.3)\n",
      "Epoch 018/030 | Loss 0.0561 | Win/lose count 0.5/91.2 (-90.7)\n",
      "Epoch 019/030 | Loss 0.0466 | Win/lose count 9.5/65.80000000000001 (-56.30000000000001)\n",
      "Epoch 020/030 | Loss 0.0395 | Win/lose count 5.5/84.79999999999998 (-79.29999999999998)\n",
      "Epoch 021/030 | Loss 0.0393 | Win/lose count 1.0/84.5 (-83.5)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.9, memory_size=2000, batch_size = 32,n_state=3)\n",
    "train_explore(agent, env, epochs_train, prefix='cnn_train_explore')\n",
    "HTML(display_videos('cnn_train_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 10.0/4.0. Average score (6.0)\n",
      "Win/lose count 9.0/8.0. Average score (3.5)\n",
      "Win/lose count 12.5/4.0. Average score (5.166666666666667)\n",
      "Win/lose count 11.0/5.0. Average score (5.375)\n",
      "Win/lose count 9.5/3.0. Average score (5.6)\n",
      "Win/lose count 9.5/4.0. Average score (5.583333333333333)\n",
      "Win/lose count 15.5/9.0. Average score (5.714285714285714)\n",
      "Win/lose count 8.0/1.0. Average score (5.875)\n",
      "Win/lose count 4.0/6.0. Average score (5.0)\n",
      "Win/lose count 3.0/0. Average score (4.8)\n",
      "Win/lose count 6.5/6.0. Average score (4.409090909090909)\n",
      "Win/lose count 5.0/3.0. Average score (4.208333333333333)\n",
      "Win/lose count 8.0/1.0. Average score (4.423076923076923)\n",
      "Win/lose count 2.0/2.0. Average score (4.107142857142857)\n",
      "Win/lose count 14.0/14.0. Average score (3.8333333333333335)\n",
      "Win/lose count 4.0/3.0. Average score (3.65625)\n",
      "Win/lose count 9.0/8.0. Average score (3.5)\n",
      "Win/lose count 11.0/4.0. Average score (3.6944444444444446)\n",
      "Win/lose count 5.0/2.0. Average score (3.6578947368421053)\n",
      "Win/lose count 10.0/3.0. Average score (3.825)\n",
      "Win/lose count 18.0/8.0. Average score (4.119047619047619)\n",
      "Win/lose count 2.5/2.0. Average score (3.9545454545454546)\n",
      "Win/lose count 8.0/1.0. Average score (4.086956521739131)\n",
      "Win/lose count 13.0/6.0. Average score (4.208333333333333)\n",
      "Win/lose count 8.5/4.0. Average score (4.22)\n",
      "Win/lose count 13.0/11.0. Average score (4.134615384615385)\n",
      "Win/lose count 18.5/10.0. Average score (4.296296296296297)\n",
      "Win/lose count 8.5/3.0. Average score (4.339285714285714)\n",
      "Win/lose count 5.5/3.0. Average score (4.275862068965517)\n",
      "Win/lose count 4.0/4.0. Average score (4.133333333333334)\n",
      "Final score: 4.133333333333334\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "test(agent,env,epochs_test,prefix='cnn_test_explore')\n",
    "HTML(display_videos('cnn_test_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__BONUS question__ Use the expert DQN from the previous question to generate some winning games. Train a model that mimicks its behavior. Compare the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to build a classifier on the memory of our agent episode to fullfil the task of taking a decision based on the current state for this we need to take a larger memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 5, 5, 32)          416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 64)          8256      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 25,380\n",
      "Trainable params: 25,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 000/030 | Loss 0.0298 | Win/lose/malus count 16.0/26.0/13.800000000000011 (-23.80000000000001)\n",
      "Epoch 001/030 | Loss 0.0111 | Win/lose/malus count 4.5/23.0/17.24999999999999 (-35.749999999999986)\n",
      "Epoch 002/030 | Loss 0.0123 | Win/lose/malus count 9.5/21.0/15.799999999999999 (-27.299999999999997)\n",
      "Epoch 003/030 | Loss 0.0202 | Win/lose/malus count 9.0/17.0/20.799999999999994 (-28.799999999999994)\n",
      "Epoch 004/030 | Loss 0.0357 | Win/lose/malus count 6.5/10.0/34.350000000000016 (-37.850000000000016)\n",
      "Epoch 005/030 | Loss 0.0239 | Win/lose/malus count 5.0/9.0/42.45000000000003 (-46.45000000000003)\n",
      "Epoch 006/030 | Loss 0.0294 | Win/lose/malus count 5.0/10.0/42.65000000000005 (-47.65000000000005)\n",
      "Epoch 007/030 | Loss 0.0280 | Win/lose/malus count 6.0/10.0/29.999999999999982 (-33.999999999999986)\n",
      "Epoch 008/030 | Loss 0.0463 | Win/lose/malus count 6.5/13.0/39.69999999999997 (-46.19999999999997)\n",
      "Epoch 009/030 | Loss 0.0359 | Win/lose/malus count 7.5/5.0/36.50000000000003 (-34.00000000000003)\n",
      "Epoch 010/030 | Loss 0.0375 | Win/lose/malus count 3.5/12.0/39.15000000000002 (-47.65000000000002)\n",
      "Epoch 011/030 | Loss 0.0275 | Win/lose/malus count 2.5/8.0/44.100000000000044 (-49.600000000000044)\n",
      "Epoch 012/030 | Loss 0.0290 | Win/lose/malus count 3.0/8.0/42.950000000000045 (-47.950000000000045)\n",
      "Epoch 013/030 | Loss 0.0565 | Win/lose/malus count 7.0/5.0/46.39999999999999 (-44.39999999999999)\n",
      "Epoch 014/030 | Loss 0.0254 | Win/lose/malus count 6.5/6.0/34.150000000000006 (-33.650000000000006)\n",
      "Epoch 015/030 | Loss 0.0474 | Win/lose/malus count 6.5/5.0/51.75000000000004 (-50.25000000000004)\n",
      "Epoch 016/030 | Loss 0.0248 | Win/lose/malus count 4.5/6.0/47.700000000000045 (-49.200000000000045)\n",
      "Epoch 017/030 | Loss 0.0340 | Win/lose/malus count 5.0/2.0/57.40000000000014 (-54.40000000000014)\n",
      "Epoch 018/030 | Loss 0.0231 | Win/lose/malus count 2.5/1.0/65.55000000000014 (-64.05000000000014)\n",
      "Epoch 019/030 | Loss 0.0333 | Win/lose/malus count 4.0/2.0/55.450000000000024 (-53.450000000000024)\n",
      "Epoch 020/030 | Loss 0.0240 | Win/lose/malus count 2.5/0/68.5000000000002 (-66.0000000000002)\n",
      "Epoch 021/030 | Loss 0.0329 | Win/lose/malus count 3.5/5.0/54.30000000000008 (-55.80000000000008)\n",
      "Epoch 022/030 | Loss 0.0484 | Win/lose/malus count 1.5/2.0/74.70000000000022 (-75.20000000000022)\n",
      "Epoch 023/030 | Loss 0.0293 | Win/lose/malus count 4.5/4.0/63.30000000000012 (-62.80000000000012)\n",
      "Epoch 024/030 | Loss 0.0367 | Win/lose/malus count 1.0/2.0/74.1000000000002 (-75.1000000000002)\n",
      "Epoch 025/030 | Loss 0.0318 | Win/lose/malus count 2.5/1.0/68.00000000000017 (-66.50000000000017)\n",
      "Epoch 026/030 | Loss 0.0292 | Win/lose/malus count 1.5/3.0/68.75000000000018 (-70.25000000000018)\n",
      "Epoch 027/030 | Loss 0.0381 | Win/lose/malus count 5.5/3.0/59.850000000000094 (-57.350000000000094)\n",
      "Epoch 028/030 | Loss 0.0384 | Win/lose/malus count 4.5/2.0/61.75000000000009 (-59.25000000000009)\n",
      "Epoch 029/030 | Loss 0.0321 | Win/lose/malus count 2.0/0/72.7500000000002 (-70.7500000000002)\n"
     ]
    }
   ],
   "source": [
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.9, memory_size=5000, batch_size = 32,n_state=3)\n",
    "train_explore(agent, env, epochs_train, prefix='cnn_train_explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "class DQN_Imitation(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        \n",
    "        model =  AdaBoostClassifier(n_estimators=100)\n",
    "        self.model = model\n",
    "        \n",
    "    def train(self, agent, env):\n",
    "        \n",
    "        for k in range(len(agent.memory.memory)//T):\n",
    "            \n",
    "            state = env.reset()\n",
    "            game_over = False\n",
    "            \n",
    "            while game_over == False:\n",
    "                action = agent.act(state)\n",
    "                prev_state = state\n",
    "                state, reward, game_over = env.act(action)\n",
    "                agent.memory.remember([state, prev_state, action , reward, game_over])\n",
    "                \n",
    "                \n",
    "        train = np.array(agent.memory.memory)\n",
    "        y = np.array(train[:,2])\n",
    "        X = train[:,1]\n",
    "        print(X)\n",
    "        print(y)\n",
    "        self.model.fit(X,y)\n",
    "    \n",
    "    def learned_act(self, s):\n",
    "        return self.model.predict(s)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.  ,  0.5 , -1.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  1.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  , -1.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 , -1.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 , -1.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ]]])\n",
      " array([[[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  1.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]]])\n",
      " array([[[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  1.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ]]])\n",
      " ...\n",
      " array([[[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  1.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.5 , -1.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  , -1.  ],\n",
      "        [ 0.  ,  0.5 , -1.  ],\n",
      "        [ 0.  ,  0.5 , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ]]])\n",
      " array([[[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  1.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.  , -1.  ],\n",
      "        [ 0.  ,  0.5 , -1.  ]]])\n",
      " array([[[ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.05,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]],\n",
      "\n",
      "       [[ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  1.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.5 ,  0.  ]],\n",
      "\n",
      "       [[ 0.  ,  0.5 ,  0.  ],\n",
      "        [ 0.  , -1.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.45,  0.  ,  0.  ],\n",
      "        [ 0.  ,  0.  ,  0.  ]]])]\n",
      "[0 2 0 ... 0 1 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5beb815cbf12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimit_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN_Imitation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimit_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-a43c1b1a3425>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, agent, env)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlearned_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\CLEAN\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\CLEAN\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m--> 110\u001b[1;33m                          y_numeric=is_regressor(self))\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\CLEAN\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\CLEAN\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\CLEAN\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "imit_agent = DQN_Imitation()\n",
    "imit_agent.train(agent,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
